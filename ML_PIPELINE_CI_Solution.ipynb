{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "iCEfxsjcf47L",
        "6rMApUs7qyxk",
        "JUVfVe5K-3Fb",
        "tWmFM7v3nmxN",
        "ZnHed4fS4fbD",
        "CjzxiSha_mdR",
        "TyxpYGf8Uyn1",
        "kgM79ZLU_sOP",
        "MHJ0TBFesM70",
        "dlukKVJu08oR",
        "PtDL6xHmMkHF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w3A6SSCXfaR"
      },
      "source": [
        "### CSE-602 Supervised Machine Learning Project\n",
        "\n",
        "### GROUP MEMBERS:\n",
        "**AYESHA NOOR KHAN (ERP 29460)**\n",
        "\n",
        "**MARYAM KHAN (ERP 08635)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-qMbtJWYo3b"
      },
      "source": [
        "**PROJECT GOAL:**\n",
        "\n",
        "The goal of the project is to identify an imbalance class dataset and make the possible solution for it using Class Balancing Solutions.\n",
        "\n",
        "**CI SOLUTIONS AND CLASSIFIERS USED:**\n",
        "\n",
        "In this dataset we will be covering three Class Imbalance Solution:\n",
        "\n",
        "1.   Resampling Method (SMOTE)\n",
        "2.   Algorithmic Method (Class Weighting and Ensemble Method Bagging | Boosting)\n",
        "3.   One-Class learning (OneClassSVM)\n",
        "\n",
        "As it is a classification problem we will be evaluating the performance metrics on 5 classification algorithms:\n",
        "\n",
        "1.   Decision Tree Classifier\n",
        "2.   KNN Classifier\n",
        "3.   Random Forest Classifier\n",
        "4.   SVM Classifier\n",
        "5.   Guassian Naive Bayes Classifier\n",
        "\n",
        "**PROPOSED METHODOLOGY:**\n",
        "\n",
        "We will be starting from our baseline dataset (imbalance) on above choosen models with best hyperparameters tuning.\n",
        "We will train our model on imbalance dataset and record the performance metrics.\n",
        "AFter that we will train our model with balance dataset and record the performance metrics,\n",
        "In the end, we will make the comparison of our baseline executed model's metrics and CI solution based executed model's metrics.\n",
        "This will give us the best possible model for our case of dataset.\n",
        "\n",
        "\n",
        "**DATASET OVERVIEW:**\n",
        "\n",
        "Of the 12,330 sessions in the dataset, 84.5% (10,422) were negative class samples that did not end with shopping, and the rest (1908) were positive class samples ending with shopping.\n",
        "\n",
        "For More Info: [View Online Shoppers Purchasing Intention Dataset Detail Information](https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ff_RbblcSkR"
      },
      "source": [
        "### INSTALL PACKAGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9KWOKfPTlRt8"
      },
      "outputs": [],
      "source": [
        "pip install pandas requests ucimlrepo Plotly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewZfJux3fglw"
      },
      "source": [
        "### IMPORT REQUIRED LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajb-lGmdkqkD"
      },
      "outputs": [],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import plotly.figure_factory as plotly\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel, RFECV\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score,StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import OneClassSVM, SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x00BvszSfs4P"
      },
      "source": [
        "###FETCH DATASET AND EXTRACT FEATURES AND TARGET CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iosVvYAsk8Y5"
      },
      "outputs": [],
      "source": [
        "load_dataset=\"\"\n",
        "X=pd.DataFrame()\n",
        "y=pd.DataFrame()\n",
        "def fetch_and_extract_dataset(dataset_id):\n",
        "    load_dataset = fetch_ucirepo(id=468)\n",
        "    # Extract features and targets\n",
        "    X = load_dataset.data.features\n",
        "    y = load_dataset.data.targets\n",
        "    # Standardizing the features\n",
        "    return load_dataset, X, y\n",
        "load_dataset,X,y=fetch_and_extract_dataset(572)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxiKDsjt3egS"
      },
      "source": [
        "In above cell, we have fetched our data directly from UCI Repo and extract the features in X variable and target class in y variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCEfxsjcf47L"
      },
      "source": [
        "###DESCRIPTION OF DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z29IQJS2fcJm"
      },
      "outputs": [],
      "source": [
        "def describe_dataset(load_dataset):\n",
        "  print(\"METADATA OF COMPANIES'S BANKRUPTCY\")\n",
        "  print(\"\")\n",
        "  print(\"Dataset Name: \",load_dataset.metadata.name)\n",
        "  print(\"Repository URL: \",load_dataset.metadata.repository_url)\n",
        "  print(\"\")\n",
        "  print(\"VARIABLES OF COMPANIES'S BANKRUPTCY\")\n",
        "  print(\"\")\n",
        "  print(load_dataset.variables)\n",
        "describe_dataset(load_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeiruzLfqrUE"
      },
      "source": [
        "###DATA CLEANING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2SK6aNb3zYo"
      },
      "source": [
        "In our case, the dataset doesnot have any null values but still we have done an approach of filling out null values with median of the X features, and verified the data cleaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxswxBBBtFSD"
      },
      "outputs": [],
      "source": [
        "def dataCleaning(X,y):\n",
        "    # Drop 'Month' column\n",
        "    X = X.drop(columns=['Month'])\n",
        "\n",
        "    # Map 'VisitorType' with numerical values\n",
        "    X['VisitorType'] = X['VisitorType'].map({'New_Visitor': 1, 'Returning_Visitor': 0, None: 0})  # assuming missing VisitorType as Returning\n",
        "\n",
        "    # Map boolean columns to 0 and 1\n",
        "    X['Weekend'] = X['Weekend'].astype(int)\n",
        "\n",
        "    # # Ensure no NA values in 'Revenue'\n",
        "    # if y['Revenue'].isna().any():\n",
        "    #     print(\"NA values found in Revenue, filling with 0 (assuming False for NA)\")\n",
        "    #     y['Revenue'] = y['Revenue'].fillna(1)\n",
        "    # y['Revenue'] = y['Revenue'].astype(int)\n",
        "\n",
        "    # Calculate the threshold as a percentage of your DataFrame's size\n",
        "    threshold_percentage = 10  # This is an example threshold percentage\n",
        "    threshold = len(y) * (threshold_percentage / 100)\n",
        "\n",
        "    # Check the distribution of 'Revenue' before deciding on fill method\n",
        "    print(y['Revenue'].value_counts())\n",
        "\n",
        "    # If a considerable amount of data is missing, consider dropping or using statistical methods to fill\n",
        "    if y['Revenue'].isna().sum() > threshold:  # Set some threshold value\n",
        "        y.dropna(subset=['Revenue'], inplace=True)\n",
        "        print(\"under threshold\")\n",
        "    else:\n",
        "        # Fill with the most common value\n",
        "        mode_value = y['Revenue'].mode()[0]\n",
        "        y['Revenue'] = y['Revenue'].fillna(mode_value)\n",
        "\n",
        "    y['Revenue'] = y['Revenue'].astype(int)\n",
        "\n",
        "    # Handling missing values - Assuming you want to handle numerical columns\n",
        "    numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "    for col in numerical_cols:\n",
        "        X[col] = X[col].fillna(X[col].median())\n",
        "\n",
        "    # Verify Data Cleaning\n",
        "    print(\"\\nVerify Data Cleaning\\n\")\n",
        "    for col in X.columns:\n",
        "        print(f\"{col} : {X[col].isnull().sum()} null values\")\n",
        "    print(f\"Revenue : {y['Revenue'].isnull().sum()} null values\")\n",
        "    return X,y\n",
        "X,y=dataCleaning(X,y)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "WADdEOGonSk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Columns that need standardization\n",
        "columns_to_scale = ['ProductRelated_Duration', 'BounceRates', 'ExitRates', 'PageValues']\n",
        "\n",
        "# Columns that need one-hot encoding\n",
        "columns_to_encode = ['OperatingSystems', 'Browser', 'Region', 'TrafficType']\n",
        "\n",
        "# Setting up the ColumnTransformer to apply transformations\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('norm', StandardScaler(), columns_to_scale),\n",
        "        ('oneHot', OneHotEncoder(), columns_to_encode)\n",
        "    ])\n",
        "\n",
        "# Create a preprocessing pipeline\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "# Apply transformations\n",
        "df_transformed = pipeline.fit_transform(X)\n",
        "\n",
        "# Get new column names correctly for the entire transformed dataset\n",
        "new_columns = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Create a new DataFrame with the transformed features\n",
        "if isinstance(df_transformed, np.ndarray):\n",
        "    X_transformed = pd.DataFrame(df_transformed, columns=new_columns)\n",
        "else:\n",
        "    X = pd.DataFrame(df_transformed.toarray(), columns=new_columns)\n",
        "\n",
        "print(X.head())"
      ],
      "metadata": {
        "id": "V6ySJB6wwPIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZkty2VmGE_5"
      },
      "source": [
        "#REUSABLE FUNCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhurEetI4NZa"
      },
      "source": [
        "We have created few resuable functions, such as confusion metrics visualization, data distribution of target class visualization, model evaulating reusable function, just to make the code clean. :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoeVev3opa7W"
      },
      "source": [
        "###CONFUSION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd1EyzG4plo9"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(ax, y_true, y_pred, metrics,clf_name, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap, ax=ax)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Predicted label')\n",
        "    ax.set_ylabel('Actual label')\n",
        "\n",
        "    roc_auc_text = f\"{metrics['roc_auc']:.2f}\" if metrics['roc_auc'] is not None else 'N/A'\n",
        "    metrics_text = f\"\"\"\n",
        "    {clf_name} Performance Metrics:\n",
        "    Accuracy  : {metrics['accuracy']:.2f}\n",
        "    Precision : {metrics['precision']:.2f}\n",
        "    Recall    : {metrics['recall']:.2f}\n",
        "    F1 Score  : {metrics['f1_score']:.2f}\n",
        "    ROC AUC   : {roc_auc_text}\n",
        "    \"\"\"\n",
        "\n",
        "    ax.text(x=0.5, y=-0.15, s=metrics_text, ha='center', va='top',\n",
        "            transform=ax.transAxes, fontsize=10, color='black', family='monospace')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN-tXllLpmCS"
      },
      "source": [
        "###DATA DISTRIBUTION PLOT VISUALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdsdyLfUpvZ6"
      },
      "outputs": [],
      "source": [
        "def plot_data_distribution(y_train, title, pie_angle):\n",
        "    plt.figure(figsize=(13, 4), facecolor=None, edgecolor=None)\n",
        "\n",
        "    # Summary data\n",
        "    total_samples = len(y_train)\n",
        "    bankrupt_1_count = np.sum(y_train == 1).values\n",
        "    bankrupt_0_count = np.sum(y_train == 0).values\n",
        "\n",
        "    # First plot (Bar Chart)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    y_train_series = pd.Series(np.ravel(y_train), name=\"Revenue\")\n",
        "    sns.countplot(x=y_train_series, palette=['#00e6ac', '#b32400'], legend=False, hue=y_train_series)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Revenue')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks([0, 1], ['Revenue 0', 'Revenue 1'])\n",
        "    # Display data summary on the plot\n",
        "    plt.text(1.2, 0.2, f\"Minority Class Revenue 1 = {bankrupt_1_count}\\nMajority Class Revenue 0 = {bankrupt_0_count}\\nTotal samples = {total_samples}\",\n",
        "             horizontalalignment='left', verticalalignment='top', transform=plt.gca().transAxes)\n",
        "\n",
        "    # Second plot (Pie Chart)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    unique, counts = np.unique(np.ravel(y_train), return_counts=True)\n",
        "    labels = ['Revenue 0', 'Revenue 1']\n",
        "    plt.pie(counts, labels=labels, autopct='%1.2f%%', startangle=pie_angle, colors=['#00e6ac', '#b32400'], explode=[0.09, 0.09])\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJDizjX9GXGc"
      },
      "source": [
        "###EVALUATE MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idGN0r6EGepy"
      },
      "outputs": [],
      "source": [
        "def evaluate_classifier(clf, clf_name, classifier_level, X_train, y_train, X_test, y_test, ax):\n",
        "    clf.fit(X_train, np.ravel(y_train))\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_prob = clf.predict_proba(X_test)[:, 1] if hasattr(clf, \"predict_proba\") else None\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred, average='binary', pos_label=1),\n",
        "        'recall': recall_score(y_test, y_pred, average='binary',pos_label=1),\n",
        "        'f1_score': f1_score(y_test, y_pred, average='binary', pos_label=1),\n",
        "        'roc_auc': roc_auc_score(y_test, y_prob) if y_prob is not None else None\n",
        "    }\n",
        "\n",
        "    # Plot the confusion matrix on the provided ax\n",
        "    plot_confusion_matrix(ax, y_test, y_pred, metrics, clf_name, title=f'{clf_name} Confusion Matrix')\n",
        "\n",
        "    return {\n",
        "        'Model Name': classifier_level + \" \" + clf_name,\n",
        "        'Accuracy': f\"{accuracy_score(y_test, y_pred):.2f}\",\n",
        "        'Precision': f\"{precision_score(y_test, y_pred, average='binary', pos_label=1):.2f}\",\n",
        "        'Recall': f\"{recall_score(y_test, y_pred, average='binary', pos_label=1):.2f}\",\n",
        "        'F1 Score': f\"{f1_score(y_test, y_pred, average='binary', pos_label=1):.2f}\",\n",
        "        'ROC AUC': f\"{roc_auc_score(y_test, y_prob):.2f}\" if y_prob is not None else 'N/A'\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju0JPDfHkcTL"
      },
      "source": [
        "###EVALUATE AND PLOT MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTOQNC4EkeNZ"
      },
      "outputs": [],
      "source": [
        "def evaluate_and_plot_classifiers(classifiers, X_train, y_train, X_test, y_test, title_suffix=\"\"):\n",
        "    num_classifiers = len(classifiers)\n",
        "    fig, axs = plt.subplots(1, num_classifiers, figsize=(num_classifiers * 5, 5))\n",
        "    fig.subplots_adjust(left=0.05, right=0.95, top=0.9, bottom=0.1, wspace=0.4, hspace=0.4)\n",
        "\n",
        "    results = []\n",
        "    for ax, (classifier_name, classifier) in zip(axs, classifiers.items()):\n",
        "        model_eval_result = evaluate_classifier(classifier, classifier_name, title_suffix,\n",
        "                                                X_train, y_train, X_test, y_test, ax)\n",
        "        results.append(model_eval_result)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY3hWyatvTHa"
      },
      "source": [
        "### EVALUATE AND PLOT BAGGING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G4jJ5z3vbkk"
      },
      "outputs": [],
      "source": [
        "def evaluate_and_plot_bagging_classifiers(X_train, X_test, y_train, y_test):\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  bagging_cls = BaggingClassifier(estimator=RandomForestClassifier(class_weight=\"balanced\", max_depth=5),n_estimators=50, random_state=42)\n",
        "\n",
        "  bagging_cls.fit(X_train, np.ravel(y_train))\n",
        "\n",
        "  # Predict on the test set\n",
        "  y_pred = bagging_cls.predict(X_test)\n",
        "  y_prob = bagging_cls.predict_proba(X_test)[:, 1] if hasattr(bagging_cls, \"predict_proba\") else None\n",
        "\n",
        "  # Calculate metrics\n",
        "  metrics = {\n",
        "      'accuracy': accuracy_score(y_test, y_pred),\n",
        "      'precision': precision_score(y_test, y_pred, average='binary', pos_label=1),\n",
        "      'recall': recall_score(y_test, y_pred, average='binary', pos_label=1),\n",
        "      'f1_score': f1_score(y_test, y_pred, average='binary', pos_label=1),\n",
        "      'roc_auc': roc_auc_score(y_test, y_prob) if y_prob is not None else None\n",
        "  }\n",
        "\n",
        "  # Plot confusion matrix\n",
        "  fig, ax = plt.subplots(figsize=(8, 5))\n",
        "  plot_confusion_matrix(ax, y_test, y_pred, metrics, \"Bagging Classifier\")\n",
        "\n",
        "  plt.show()\n",
        "  return {\n",
        "      'Model Name': 'Bagging Classifer with Random Forest',\n",
        "      'Accuracy': f\"{metrics['accuracy']:.2f}\",\n",
        "      'Precision': f\"{metrics['precision']:.2f}\",\n",
        "      'Recall': f\"{metrics['recall']:.2f}\",\n",
        "      'F1 Score': f\"{metrics['f1_score']:.2f}\",\n",
        "      'ROC AUC': f\"{metrics['roc_auc']:.2f}\" if metrics['roc_auc'] is not None else 'N/A'\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQFVlsLwvl5w"
      },
      "source": [
        "We have used bagging model with Random Forest Classifer having balanced class weight and max depth of tree to 5 to avoid overfitting if I increase the max depth of tree the model start increasing the recall and decreasing the precision, hence getting overfitting in testing case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7mAmKbPwrmb"
      },
      "source": [
        "### EVALUATE AND PLOT BOOSTING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez7jSC1bwrmc"
      },
      "outputs": [],
      "source": [
        "def evaluate_and_plot_boosting_classifiers(X_train_smote, X_test, y_train_smote, y_test):\n",
        "    # Evaluate AdaBoost Classifier\n",
        "    boosting_cls = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=8), n_estimators=50, random_state=42)\n",
        "    boosting_cls.fit(X_train_smote, np.ravel(y_train_smote))\n",
        "    y_pred_boosting = boosting_cls.predict(X_test)\n",
        "    y_prob_boosting = boosting_cls.predict_proba(X_test)[:, 1] if hasattr(boosting_cls, \"predict_proba\") else None\n",
        "\n",
        "    metrics_boosting = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred_boosting),\n",
        "        'precision': precision_score(y_test, y_pred_boosting, average='binary', pos_label=1),\n",
        "        'recall': recall_score(y_test, y_pred_boosting, average='binary', pos_label=1),\n",
        "        'f1_score': f1_score(y_test, y_pred_boosting, average='binary', pos_label=1),\n",
        "        'roc_auc': roc_auc_score(y_test, y_prob_boosting) if y_prob_boosting is not None else 'N/A'\n",
        "    }\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    plot_confusion_matrix(ax, y_test, y_pred_boosting, metrics_boosting, \"AdaBoost Classifier\")\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'Model Name': 'Boosting Classifier with AdaBoost',\n",
        "        'Accuracy': f\"{metrics_boosting['accuracy']:.2f}\",\n",
        "        'Precision': f\"{metrics_boosting['precision']:.2f}\",\n",
        "        'Recall': f\"{metrics_boosting['recall']:.2f}\",\n",
        "        'F1 Score': f\"{metrics_boosting['f1_score']:.2f}\",\n",
        "        'ROC AUC': f\"{metrics_boosting['roc_auc']:.2f}\" if metrics_boosting['roc_auc'] is not None else 'N/A'\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEd2C2pawrmd"
      },
      "source": [
        "We have used boosting model with adaboost and decition tree weak classifiers having max depth of tree to 8 to avoid overfitting if I increase the max depth of tree the model start increasing the recall and decreasing the precision, and taking lot of complexity and computing power hence getting overfitting in testing case.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFSHlIZw0GDl"
      },
      "source": [
        "### EVALUATE AND PLOT ONE CLASS SVM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0BkPGP00GDm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_and_plot_one_class_svm(X_train, X_test, y_train, y_test):\n",
        "    # Split the dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Ensure that the test set has the same column order as the training set\n",
        "    X_test = X_test[X_train.columns]\n",
        "\n",
        "    # Parameters for OneClassSVM\n",
        "    nu = 0.1  # Upper bound on the fraction of training errors\n",
        "    gamma = 'scale'  # Kernel coefficient\n",
        "    kernel = 'rbf'  # Type of kernel\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Verify dimensions match\n",
        "    if len(y_train) != X_train_scaled.shape[0]:\n",
        "        raise ValueError(\"Mismatch in the number of rows between y_train and X_train_scaled\")\n",
        "\n",
        "    # Filter training data to only non-bankrupt class\n",
        "    non_bankrupt_data = X_train_scaled[y_train['Revenue'] == 0]\n",
        "    # Use boolean indexing directly\n",
        "\n",
        "    # Initialize OneClassSVM\n",
        "    oc_svm = OneClassSVM(kernel=kernel, gamma=gamma, nu=nu)\n",
        "    oc_svm.fit(non_bankrupt_data)  # Training only on the non-bankrupt data\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = oc_svm.predict(X_test_scaled)\n",
        "\n",
        "    # Adjust predictions to match the expected binary labels\n",
        "    y_pred_binary = (y_pred == -1).astype(int)  # Convert -1 (outlier) to 1 (bankrupt)\n",
        "    y_test_binary = (y_test == 1).astype(int)  # 1 for bankrupt, 0 for non-bankrupt\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics_svm = {\n",
        "        'accuracy': accuracy_score(y_test_binary, y_pred_binary),\n",
        "        'precision': precision_score(y_test_binary, y_pred_binary,average='binary', pos_label=1),\n",
        "        'recall': recall_score(y_test_binary, y_pred_binary,average='binary', pos_label=1),\n",
        "        'f1_score': f1_score(y_test_binary, y_pred_binary,average='binary', pos_label=1),\n",
        "        'roc_auc': roc_auc_score(y_test_binary, oc_svm.decision_function(X_test_scaled),average='weighted') if any(y_pred_binary) else 'N/A'\n",
        "    }\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    plot_confusion_matrix(ax, y_test_binary, y_pred_binary, metrics_svm, \"OneClass SVM Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'Model Name': 'OneClass SVM with RBF Kernel',\n",
        "        'Accuracy': f\"{metrics_svm['accuracy']:.2f}\",\n",
        "        'Precision': f\"{metrics_svm['precision']:.2f}\",\n",
        "        'Recall': f\"{metrics_svm['recall']:.2f}\",\n",
        "        'F1 Score': f\"{metrics_svm['f1_score']:.2f}\",\n",
        "        'ROC AUC': f\"{metrics_svm['roc_auc']:.2f}\" if metrics_svm['roc_auc'] != 'N/A' else 'N/A'\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvwmSdNH0GDm"
      },
      "source": [
        "ABove is the fucntion for plotting and evaluating oneclass svm with kernel rbf and gamme \"scale\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rMApUs7qyxk"
      },
      "source": [
        "#EXPLORATORY DATA ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-Jn5Esx4v79"
      },
      "source": [
        "This is an important stage in our model building goal, as our Dataset of **\"Online Shoppers Purchasing Intention Dataset\"** is having 17 features and many of features caa be corelated, extra, can cause variance etc, for solution we want to have deeper look at our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fadgkKGhPkC"
      },
      "source": [
        "###IS IT IMBALANCE CLASS DATASET ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOXSpISX5RaX"
      },
      "source": [
        "First we have to find whether our dataset is actually class imabalanced or not, as we are going to start our model training first thing we should check about the class distribution ration. So here we go:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo1mPc6sxxKe"
      },
      "outputs": [],
      "source": [
        "def check_imbalance(y, threshold=20):\n",
        "    print(\"\\n\")\n",
        "    class_distribution = y.value_counts(normalize=True) * 100\n",
        "\n",
        "    plt.figure(figsize=(7, 4), facecolor=None, edgecolor=None)\n",
        "    ax = class_distribution.plot(kind=\"bar\", color=['#00e6ac', '#b32400'])\n",
        "    ax.set_xlabel(\"Revenue\")\n",
        "    ax.set_ylabel(\"Frequency\")\n",
        "    ax.set_title(\"Is Target Class Imbalance?\")\n",
        "\n",
        "    is_imbalanced = class_distribution.min() < threshold\n",
        "    imbalance_text = f\"Yes, Dataset Have Imbalanced Class Distribution.\" if is_imbalanced else f\"No, Dataset Have Balanced Class Distribution.\"\n",
        "    ax.text(1.1, 0.5, imbalance_text, horizontalalignment='left', verticalalignment='top', transform=ax.transAxes, fontsize=12)\n",
        "\n",
        "    plt.show()\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNsXZ9ahg7kc"
      },
      "source": [
        "###DATASET DESCRIPTIVE STATISTICS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLDvWuSF6e7C"
      },
      "source": [
        "Next we have done a central way of viewing our dataset statistics , variability and distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYPfkGiWtet2"
      },
      "outputs": [],
      "source": [
        "def describe_dataset_stats(X, y):\n",
        "  print(\"Data Statistics :\")\n",
        "  print(X.describe())\n",
        "  print(y)\n",
        "describe_dataset_stats(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3oTMhOxi26h"
      },
      "source": [
        "###COORELATION HEATMAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-ixizW670TE"
      },
      "source": [
        "Then we have plot a Heatmap for our dataset which helps us to identify the correlation between our features as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VFztd8DBArY"
      },
      "outputs": [],
      "source": [
        "def plot_correlation_heatmap(X):\n",
        "  correlation_matrix = X.corr()\n",
        "  plt.figure(figsize = (5,5))\n",
        "  sns.heatmap(correlation_matrix, cmap = 'coolwarm',linewidths=0.5,linecolor='grey')\n",
        "  plt.xticks(fontsize=9)\n",
        "  plt.yticks(fontsize=9)\n",
        "  plt.show()\n",
        "\n",
        "plot_correlation_heatmap(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLEcnFEH9mvM"
      },
      "source": [
        "**HEATMAP OBSERVATION:** From above heatmap it is clearly visible that:\n",
        "\n",
        "*   BounceRates, ProductRelated, ProductRelated_Duration, and ExitRates are -ve correlated.\n",
        "*   Administrative, Administrative_Duration, Informational, Informational_Duration, ProductRelated and ProductRelated_Duration are strongly correlated.\n",
        "*   We can use ProductRelated_Duration instead of both ProductRelated and ProductRelated_Duration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUVfVe5K-3Fb"
      },
      "source": [
        "# DIMENSIONALITY REDUCTION OR FEATURE SELECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZehRzJ-Cwr7"
      },
      "source": [
        "Exploring dimensionality reduction using PCA to predict the data on less features as possible according to Occam's Razor Rule and other methods if possible\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50OqUpod7jgv"
      },
      "source": [
        "###PCA FOR DIMENSIONALITY REDUCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvnHaS6uaVbE"
      },
      "outputs": [],
      "source": [
        "def do_pca(X_features):\n",
        "    pca = PCA().fit(X_features)\n",
        "    plt.figure(figsize = (5,4))\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "    plt.xlabel('Number of components')\n",
        "    plt.ylabel('Cumulative sum - Explained Variance')\n",
        "    #fitting PCA on our X\n",
        "    pca = PCA(n_components=0.99).fit(X_features)\n",
        "    print(f'Best value of n_component for 99% variance : {pca.n_components_}')\n",
        "\n",
        "    pca = PCA(n_components=0.95).fit(X_features)\n",
        "    print(f'Best value of n_component for 95% variance : {pca.n_components_}')\n",
        "\n",
        "    pca = PCA(n_components=0.90).fit(X_features)\n",
        "    print(f'Best value of n_component for 90% variance : {pca.n_components_}')\n",
        "\n",
        "    pca = PCA(n_components=0.85).fit(X_features)\n",
        "    print(f'Best value of n_component for 85% variance : {pca.n_components_}')\n",
        "\n",
        "    pca = PCA(n_components=0.80).fit(X_features)\n",
        "    print(f'Best value of n_component for 80% variance : {pca.n_components_}')\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooicez5pkOOm"
      },
      "source": [
        "###FEATURE SELECTION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TScg0jcHMVJ"
      },
      "source": [
        "We have also tried feature selection with random forest and SelecFromModel to give the top 8 features of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGrRvDC_oYsU"
      },
      "outputs": [],
      "source": [
        "model = RandomForestClassifier(n_estimators=100)\n",
        "model.fit(X, np.ravel(y))\n",
        "\n",
        "model_selector = SelectFromModel(model, prefit=True,threshold='median', max_features=8)\n",
        "model_selector.fit(X, np.ravel(y))\n",
        "X_selected = model_selector.transform(X)\n",
        "\n",
        "selected_features = X.columns[model_selector.get_support()]\n",
        "\n",
        "feature_importance = model.feature_importances_\n",
        "indices = np.argsort(feature_importance)[-8:]  #Top 20 features\n",
        "\n",
        "# Generate some random colors for demonstration purposes\n",
        "colors = np.linspace(0, 1, len(indices))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title('Top 8 Feature Importances')\n",
        "\n",
        "# Plot bar chart with gradient colors\n",
        "bars = plt.barh(range(len(indices)), feature_importance[indices], color=plt.cm.coolwarm(colors), align='center')\n",
        "\n",
        "# Set y-axis labels\n",
        "plt.yticks(range(len(indices)), [X.columns[i] for i in indices])\n",
        "\n",
        "# Add color bar for reference\n",
        "cbar = plt.colorbar(plt.cm.ScalarMappable(cmap='coolwarm'))\n",
        "cbar.set_label('Gradient')\n",
        "\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWmFM7v3nmxN"
      },
      "source": [
        "#TRAINING AND TESTING DATA SPLITTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYCQnogDKBC2"
      },
      "source": [
        "We have split our 80% data into training set and 20% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrqe5ku-L9YK"
      },
      "outputs": [],
      "source": [
        "def split_data(X,y):\n",
        "  print(\"\\nDATA SPLITTING INTO TRAINING AND TESTING SETS\\n\")\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42 )\n",
        "  plot_data_distribution(y_train,'Distribution of Revenue Classes Before CI Solution',180)\n",
        "  print(\"\\n\")\n",
        "  return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnHed4fS4fbD"
      },
      "source": [
        "#MODELS HYPERPARAMETER TUNING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQXqaufaKUTx"
      },
      "source": [
        "For each selected model, we want the best hyperparamters on which the F1 score is best, for that we have made a reusable function for all 5 classifers hyper tuning. These function will give me the best hyper params of the classifiers for out dataset. We will call these fucntion later in our code when we will be running a baseline execution of all models.\n",
        "\n",
        "**How it works?**\n",
        "\n",
        "For our classifier, we have made set of all possible params and execute them one by one and the params by which our dataset has high F1 score is selected to be the best params and returned by model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUUrCb4o4m5v"
      },
      "source": [
        "### DECISION TREE CLASSIFIER TUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiCSN6imlUoZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def decision_tree_tuning(X_train, y_train, X_test, y_test, max_depth=None, criterion=None, min_samples_split=None):\n",
        "    # Setting default parameters if None are provided\n",
        "    if max_depth is None:\n",
        "        max_depth = [10,20,25,30,35,40,50]\n",
        "    if criterion is None:\n",
        "        criterion = ['gini', 'entropy', 'log_loss']\n",
        "    if min_samples_split is None:\n",
        "        min_samples_split = [10,20,30,40]\n",
        "\n",
        "    best_params = {}\n",
        "    best_f1 = 0\n",
        "\n",
        "    # Iterating through all combinations of parameters\n",
        "    for criteria in criterion:\n",
        "        for depth in max_depth:\n",
        "            for min_samples in min_samples_split:\n",
        "                dt = DecisionTreeClassifier(max_depth=depth, criterion=criteria, min_samples_split=min_samples)\n",
        "                dt.fit(X_train, y_train)\n",
        "                y_pred = dt.predict(X_test)\n",
        "                f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "                # Update best parameters if current F1 score is higher\n",
        "                if f1 > best_f1:\n",
        "                    best_f1 = f1\n",
        "                    best_params = {\n",
        "                        'criterion': criteria,\n",
        "                        'max_depth': depth,\n",
        "                        'min_samples_split': min_samples\n",
        "                    }\n",
        "\n",
        "    return {'best_params': best_params, 'best_f1_score': best_f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP_h5d5u4zhX"
      },
      "source": [
        "### KNN CLASSIFIER TUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE7X8CkLBd1v"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def knn_tuning(X_train, y_train, X_test, y_test, n_neighbors=None, weights=None, metric=None):\n",
        "    # Set default parameters if None are provided\n",
        "    if n_neighbors is None:\n",
        "        n_neighbors = [5,10,15,20,30,50,100,200]\n",
        "    if weights is None:\n",
        "        weights = ['uniform', 'distance']\n",
        "    if metric is None:\n",
        "        metric = ['euclidean', 'manhattan', 'minkowski']\n",
        "\n",
        "    best_params = {}\n",
        "    best_f1 = 0\n",
        "\n",
        "    # Iterating through all combinations of parameters\n",
        "    for neighbor in n_neighbors:\n",
        "        for weight in weights:\n",
        "            for m in metric:\n",
        "                knn = KNeighborsClassifier(n_neighbors=neighbor, metric=m)\n",
        "                knn.fit(X_train, np.ravel(y_train))\n",
        "                y_pred = knn.predict(X_test)\n",
        "                f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "                # Update best parameters if current F1 score is higher\n",
        "                if f1 > best_f1:\n",
        "                    best_f1 = f1\n",
        "                    best_params = {\n",
        "                        'n_neighbors': neighbor,\n",
        "                        'weights': weight,\n",
        "                        'metric': m\n",
        "                    }\n",
        "\n",
        "    return {'best_params': best_params, 'best_f1_score': best_f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2o40L71444j"
      },
      "source": [
        "### RANDOM FOREST CLASSIFIER TUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3UH3zb8zBxy"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def random_forest_tuning(X_train, y_train, X_test, y_test, n_estimators=None, criterion=None):\n",
        "    if n_estimators is None:\n",
        "        n_estimators = [5, 10, 15, 20, 30, 50, 100, 150, 200, 250, 300, 400, 500]\n",
        "    if criterion is None:\n",
        "        criterion = ['entropy', 'gini']\n",
        "\n",
        "    best_params = {}\n",
        "    best_f1 = 0\n",
        "    best_model = None\n",
        "\n",
        "    for c in criterion:\n",
        "        for estimator in n_estimators:\n",
        "            rf = RandomForestClassifier(n_estimators=estimator, criterion=c)\n",
        "            rf.fit(X_train, np.ravel(y_train))\n",
        "            y_pred = rf.predict(X_test)\n",
        "            f1 = f1_score(y_test, y_pred, average='macro')\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_params = {'n_estimators': estimator, 'criterion': c}\n",
        "                best_model = rf\n",
        "\n",
        "    return {'best_params': best_params, 'best_f1_score': best_f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yDs9dc18gp0"
      },
      "source": [
        "###SVM CLASSIFIER TUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SSj3k-s18vF"
      },
      "outputs": [],
      "source": [
        "def svm_tuning(X_train, y_train, X_test, y_test, C_values=None, kernels=None):\n",
        "    if C_values is None:\n",
        "        C_values = [10]\n",
        "    if kernels is None:\n",
        "        kernels = ['rbf']\n",
        "\n",
        "    best_c = None\n",
        "    best_kernel = None\n",
        "    best_f1 = 0\n",
        "    best_model = None\n",
        "\n",
        "    # Iterate through all combinations of the specified parameters\n",
        "    for C in C_values:\n",
        "        for kernel in kernels:\n",
        "            model = SVC(C=C, kernel=kernel, probability=True)\n",
        "            model.fit(X_train, np.ravel(y_train))\n",
        "            y_pred = model.predict(X_test)\n",
        "            f1 = f1_score(y_test, y_pred, average='macro')  # Calculate F1 score\n",
        "\n",
        "            # Update the best parameters if the current F1 score is higher\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_c = C\n",
        "                best_kernel = kernel\n",
        "                best_model = model\n",
        "\n",
        "    return {\n",
        "        'best_params': {\n",
        "            'C': best_c,\n",
        "            'kernel': best_kernel\n",
        "        },\n",
        "        'best_f1_score': best_f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72Ld84Ns96-J"
      },
      "source": [
        "###GNBAYES CLASSIFIER TUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmxtvsW79Q08"
      },
      "outputs": [],
      "source": [
        "def gn_bayes_tuning(X_train, y_train, X_test, y_test):\n",
        "    # Initialize and train the Gaussian Naive Bayes model\n",
        "    model = GaussianNB()\n",
        "    model.fit(X_train, np.ravel(y_train))\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    best_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    return { 'best_params': { },'best_f1_score': best_f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjzxiSha_mdR"
      },
      "source": [
        "#MODEL BUILDING WITH BEST HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s04eBoGpLSN_"
      },
      "source": [
        "Now here we will be getting best hyperparams for the models on which the we get best F1 scores as for for imbalanced dataset, precision and recall matters, whereas F1 score is the average of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruEfNlM1_zOY"
      },
      "outputs": [],
      "source": [
        "def model_tuning(X_train, y_train, X_test, y_test):\n",
        "  print(\"\\nMODEL HYPERPARAMETER TUNING\\n\")\n",
        "  # Initialize hyperparameter tuning objects\n",
        "  decision_tree_best_hyperparam=decision_tree_tuning(X_train, y_train, X_test, y_test)['best_params']\n",
        "  knn_best_hyperparam=knn_tuning(X_train, y_train, X_test, y_test)['best_params']\n",
        "  random_forest_best_hyperparam=random_forest_tuning(X_train, y_train, X_test, y_test)['best_params']\n",
        "  svm_best_hyperparam=svm_tuning(X_train, y_train, X_test, y_test)['best_params']\n",
        "  gnbayes_best_hyperparam=gn_bayes_tuning(X_train, y_train, X_test, y_test)['best_params'] # Gaussian Naive Bayes typically does not need hyperparameter tuning\n",
        "\n",
        "  # Print best parameters (optional, for verification)\n",
        "  print(\"Decision Tree Best Params:\", decision_tree_best_hyperparam)\n",
        "  print(\"\\nKNN Best Params:\", knn_best_hyperparam)\n",
        "  print(\"\\nRandom Forest Best Params:\", random_forest_best_hyperparam)\n",
        "  print(\"\\nSVM Best Params:\", svm_best_hyperparam)\n",
        "\n",
        "  classifiers = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(**decision_tree_best_hyperparam),\n",
        "    \"KNN\": KNeighborsClassifier(**knn_best_hyperparam),\n",
        "    \"Random Forest\": RandomForestClassifier(**random_forest_best_hyperparam),\n",
        "    \"SVM\": SVC(**svm_best_hyperparam, probability=True), #prob true for ROC\n",
        "    \"Gaussian Naive Bayes\": GaussianNB()  # Gaussian Naive Bayes typically does not need parameter tuning\n",
        "  }\n",
        "  print(\"\\n\")\n",
        "  return classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gpo4vDvLi_7"
      },
      "source": [
        "Above code will shows best hyperparams for all classiers in our dataset.\n",
        "But remember, Guassian Naive Bayes doesn't support any hyperparam so it will be just executed simlply."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyxpYGf8Uyn1"
      },
      "source": [
        "#BASELINE EXECUTION OF MODEL ON IMBALANCED DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS3dGQzkMWN3"
      },
      "source": [
        "Now we will be initializing the classifiers with best hyperparams.\n",
        "\n",
        "Then we will be utilizing our reusable `evaluate_and_plot_classifiers` method to train our all 5 models and plot their results with confusion matrix and performance metrics (remember we will be having concern with precision and recall most in our imbalance class case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enNQjnOyEaxZ"
      },
      "outputs": [],
      "source": [
        "def evaluate_baseline_model(classifiers,X_train, y_train, X_test, y_test,title_suffix):\n",
        "  print(\"\\nBASELINE MODEL EVALUATION RESULTS\\n\")\n",
        "  baseline_results_df = evaluate_and_plot_classifiers(classifiers,X_train,y_train,X_test,y_test,title_suffix=\"Baseline\")\n",
        "  baseline_model_evaluation_results_df = pd.DataFrame(baseline_results_df)\n",
        "  print(\"\\n\")\n",
        "  return baseline_model_evaluation_results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgM79ZLU_sOP"
      },
      "source": [
        "#CLASS BALANCING WITH RESAMPLING METHOD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38nWxAPPawKH"
      },
      "source": [
        "As our performance metric are not overall good, we need to move forward with some class balancing solution, so first we will be trying SMOTE Resmapling Method. Let's get started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMyPQYbBokJj"
      },
      "source": [
        "###SMOTE RESAMPLING\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKPd0vu2ASLA"
      },
      "outputs": [],
      "source": [
        "def smote_resampling(X_train, y_train):\n",
        "  print(\"\\nSMOTE RESAMPLING\\n\")\n",
        "  smote = SMOTE(random_state=42)\n",
        "  X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "  plot_data_distribution(y_train_smote,'Distribution of Revenue Classes After SMOTE Resampling',90)\n",
        "  print(\"\\n\")\n",
        "  return X_train_smote, y_train_smote"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6E79Uf4bCCZ"
      },
      "source": [
        "SMOTE have done the class balancing by creating synthetic sampling of minority data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikYCNzgqDt6x"
      },
      "source": [
        "### 1. SMOTE + PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaIZG59ykP8n"
      },
      "outputs": [],
      "source": [
        "def smote_with_pca(X_train_smote):\n",
        "  print(\"\\nPRINCIPAL COMPOENENT ANALYSIS\\n\")\n",
        "  do_pca(X_train_smote)\n",
        "  print(\"\\n\")\n",
        "\n",
        "def smote_pca_transform(X_train_smote,X_test):\n",
        "  X_train_smote_PCA = PCA(2).fit_transform(X_train_smote)\n",
        "  pca = PCA(n_components=2).fit(X_train_smote)\n",
        "  X_test_PCA = pca.transform(X_test)\n",
        "  return X_train_smote_PCA,X_test_PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orH6uoivF24q"
      },
      "source": [
        "Above results of PCA shows that 2 components are needed for the 99% variance in data. It can capture 99% of info in the dataset which can be acheive from top 2 features.\n",
        "\n",
        "\n",
        "Now I have transform my smote X train features with PCA top 2 features, thus reducing the 17 features dimensionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z9omADEfriM"
      },
      "source": [
        "###MODELS EVALUTATION WITH CI SOLUTION SMOTE + PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kp1HN7hcbYR"
      },
      "source": [
        "Now we will use our `evaluate_and_plot_classifiers` method to train and evaluate the SMOTE model along with PCA dimensionality reduction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9nmqQLNfz3X"
      },
      "outputs": [],
      "source": [
        "def evaluate_smote_with_pca(classifiers,X_train_smote_PCA, y_train_smote, X_test_PCA, y_test,title_suffix):\n",
        "  print(\"\\nSMOTE WITH PCA MODEL EVALUATION\\n\")\n",
        "  smote_pca_results_df = evaluate_and_plot_classifiers(classifiers,X_train_smote_PCA, y_train_smote, X_test_PCA, y_test,title_suffix)\n",
        "  smote_pca_model_evaluation_results_df = pd.DataFrame(smote_pca_results_df)\n",
        "  print(\"\\n\")\n",
        "  return smote_pca_model_evaluation_results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb5PPRL2GViS"
      },
      "source": [
        "###MODELS EVALUTATION WITH CI SOLUTION SMOTE ONLY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lQVbGz3GVid"
      },
      "source": [
        "Now we will use our `evaluate_and_plot_classifiers` method to train and evaluate the SMOTE model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHqbsQnzGVid"
      },
      "outputs": [],
      "source": [
        "def evaluate_smote(classifiers,X_train_smote, y_train_smote, X_test, y_test,title_suffix):\n",
        "  print(\"\\nSMOTE MODEL EVALUATION\\n\")\n",
        "  smote_results_df = evaluate_and_plot_classifiers(classifiers,X_train_smote, y_train_smote, X_test, y_test,title_suffix)\n",
        "  smote_model_evaluation_results_df = pd.DataFrame(smote_results_df)\n",
        "  print(\"\\n\")\n",
        "  return smote_model_evaluation_results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHJ0TBFesM70"
      },
      "source": [
        "# ALGORITHMIC METHOD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN4pBIOHsbY1"
      },
      "source": [
        "### BAGGING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC-AzdSctXN6"
      },
      "outputs": [],
      "source": [
        "def evaluate_bagging_model(X_train, y_train, X_test, y_test,title_suffix):\n",
        "  print(\"\\nBAGGING MODEL EVALUATION RESULTS\\n\")\n",
        "  bagging_results_df = evaluate_and_plot_bagging_classifiers(X_train,y_train,X_test,y_test)\n",
        "  bagging_model_evaluation_results_df = pd.DataFrame([bagging_results_df])\n",
        "  print(\"\\n\")\n",
        "  return bagging_model_evaluation_results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFnj0yoDsqL0"
      },
      "source": [
        "Calling the bagging model reusable function in above code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6mCzrgAsmAi"
      },
      "source": [
        "### BOOSTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0FLMK1xy3i8"
      },
      "outputs": [],
      "source": [
        "def evaluate_boosting_model(X_train_smote, X_test, y_train_smote, y_test,title_suffix):\n",
        "  print(\"\\nBOOSTING MODEL EVALUATION RESULTS\\n\")\n",
        "  boosting_results_df = evaluate_and_plot_boosting_classifiers(X_train_smote, X_test, y_train_smote, y_test)\n",
        "  boosting_model_evaluation_results_df = pd.DataFrame([boosting_results_df])\n",
        "  print(\"\\n\")\n",
        "  return boosting_model_evaluation_results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlukKVJu08oR"
      },
      "source": [
        "# One Class Learning Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0woBaQp08oR"
      },
      "source": [
        "### OneCLass SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I_l6Rya08oS"
      },
      "outputs": [],
      "source": [
        "def evaluate_oneclass_svm_model(X_train, y_train, X_test, y_test,title_suffix):\n",
        "  print(\"\\nOne Class SVM MODEL EVALUATION RESULTS\\n\")\n",
        "  oneclass_results_df = evaluate_and_plot_one_class_svm(X_train,y_train,X_test,y_test)\n",
        "  oneclass_model_evaluation_results_df = pd.DataFrame([oneclass_results_df])\n",
        "  print(\"\\n\")\n",
        "  return oneclass_model_evaluation_results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6urRl1Yd08oS"
      },
      "source": [
        "Calling the OneClass SVM model reusable function in above code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtDL6xHmMkHF"
      },
      "source": [
        "# COMPARISON OF BASELINE AND SMOTE MODEL PERFORMANCE METRICS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23KzFc3nG_zv"
      },
      "outputs": [],
      "source": [
        "def compare_model_performance(baseline_model_evaluation_results_df, smote_model_evaluation_results_df, smote_pca_model_evaluation_results_df,bagging_model_evaluation_results_df,boosting_model_evaluation_results_df,oneclass_model_evaluation_results_df):\n",
        "  print(\"\\nCOMPARING MODEL PERFORMANCE\\n\")\n",
        "  combined_df = pd.concat([baseline_model_evaluation_results_df, smote_model_evaluation_results_df, smote_pca_model_evaluation_results_df,bagging_model_evaluation_results_df,boosting_model_evaluation_results_df,oneclass_model_evaluation_results_df], ignore_index=True)\n",
        "  table = combined_df.to_markdown(index=False)\n",
        "  print(table)\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4enslOvrmijh"
      },
      "source": [
        "# AUC-ROC Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66LMQ3J9HKNV"
      },
      "outputs": [],
      "source": [
        "def plot_roc_curve(classifiers,X_train_smote, y_train_smote,X_test,y_test):\n",
        "  print(\"\\nAUC ROC CURVE - MODEL WITH BEST AREA UNDER CURVE\\n\")\n",
        "  plt.figure(figsize=(8, 6))  # Smaller than the original (10, 8)\n",
        "  for name, clf in classifiers.items():\n",
        "      # Train the model\n",
        "      clf.fit(X_train_smote, np.ravel(y_train_smote))\n",
        "      # Predict probabilities\n",
        "      if hasattr(clf, \"predict_proba\"):\n",
        "          y_probs = clf.predict_proba(X_test)[:, 1]\n",
        "      else:  # Use decision function if predict_proba is not available\n",
        "          y_probs = clf.decision_function(X_test)\n",
        "      # Compute ROC curve and AUC\n",
        "      fpr, tpr, _ = roc_curve(np.ravel(y_test), y_probs)\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      # Plot\n",
        "      plt.plot(fpr, tpr, label=f'{name} (area = {roc_auc:.2f})')\n",
        "\n",
        "  plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('AUC-ROC Curve (With Balanced Classes)')\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### F1 Scores Comparison"
      ],
      "metadata": {
        "id": "wbgwSBlFmAdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_f1_scores(baseline_model_evaluation_results_df, smote_model_evaluation_results_df, smote_pca_model_evaluation_results_df, bagging_model_evaluation_results_df, boosting_model_evaluation_results_df, oneclass_model_evaluation_results_df):\n",
        "  # Create a list of model names\n",
        "  model_names = list(baseline_model_evaluation_results_df['Model Name']) + list(smote_model_evaluation_results_df['Model Name']) + list(smote_pca_model_evaluation_results_df['Model Name']) + list(bagging_model_evaluation_results_df['Model Name']) + list(boosting_model_evaluation_results_df['Model Name']) + list(oneclass_model_evaluation_results_df['Model Name'])\n",
        "\n",
        "  # Create a list of F1 scores\n",
        "  f1_scores = list(baseline_model_evaluation_results_df['F1 Score']) + list(smote_model_evaluation_results_df['F1 Score']) + list(smote_pca_model_evaluation_results_df['F1 Score']) + list(bagging_model_evaluation_results_df['F1 Score']) + list(boosting_model_evaluation_results_df['F1 Score']) + list(oneclass_model_evaluation_results_df['F1 Score'])\n",
        "\n",
        "  # Create a bar chart\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.bar(model_names, f1_scores)\n",
        "  plt.xlabel('Model Name')\n",
        "  plt.ylabel('F1 Score')\n",
        "  plt.title('F1 Scores Comparison')\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "9vJfJpiyhPKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ8ST5k0mnXq"
      },
      "source": [
        "# ML PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX3w8TdlM3Tj"
      },
      "outputs": [],
      "source": [
        "def ml_pipeline():\n",
        "  check_imbalance(y['Revenue'])\n",
        "  #Splitting Data\n",
        "  X_train, X_test, y_train, y_test= split_data(X,y)\n",
        "  #Model HyperParameter Tuning\n",
        "  classifiers = model_tuning(X_train, y_train, X_test, y_test)\n",
        "  #Baseline Model Execution On Imbalanced Datasets\n",
        "  baseline_model_evaluation_results_df = evaluate_baseline_model(classifiers,X_train,y_train,X_test,y_test,title_suffix=\"Baseline\")\n",
        "  #Smote Resampling\n",
        "  X_train_smote, y_train_smote= smote_resampling(X_train,y_train)\n",
        "  #Testing PCA Analysis\n",
        "  smote_with_pca(X_train_smote)\n",
        "  # X tranformation with Principal Components\n",
        "  X_train_smote_PCA,X_test_PCA=smote_pca_transform(X_train_smote,X_test)\n",
        "  #Resampling Model Execution On Balanced Datasets (SMOTE+PCA)\n",
        "  smote_pca_model_evaluation_results_df = evaluate_smote_with_pca(classifiers,X_train_smote_PCA, y_train_smote, X_test_PCA, y_test,title_suffix=\"(SMOTE + PCA)\")\n",
        "  #Resampling Model Execution On Balanced Datasets (SMOTE)\n",
        "  smote_model_evaluation_results_df = evaluate_smote(classifiers,X_train_smote, y_train_smote, X_test, y_test,title_suffix=\"(SMOTE)\")\n",
        "  #Bagging Model Execution\n",
        "  bagging_model_evaluation_results_df = evaluate_bagging_model(X_train, y_train, X_test, y_test,title_suffix=\"Bagging\")\n",
        "  #Boosting Model Execution\n",
        "  boosting_model_evaluation_results_df = evaluate_boosting_model(X_train_smote, X_test, y_train_smote, y_test,title_suffix=\"Boosting\")\n",
        "  #OneClass SVM Model Execution\n",
        "  oneclass_model_evaluation_results_df = evaluate_oneclass_svm_model(X_train, y_train, X_test, y_test,title_suffix=\"OneClass SVM\")\n",
        "  #plot f1 scores\n",
        "  plot_f1_scores(baseline_model_evaluation_results_df, smote_model_evaluation_results_df, smote_pca_model_evaluation_results_df, bagging_model_evaluation_results_df, boosting_model_evaluation_results_df, oneclass_model_evaluation_results_df)\n",
        "  #Comparison of Model Performance\n",
        "  compare_model_performance(baseline_model_evaluation_results_df, smote_model_evaluation_results_df, smote_pca_model_evaluation_results_df,bagging_model_evaluation_results_df,boosting_model_evaluation_results_df,oneclass_model_evaluation_results_df)\n",
        "  #ROC Curve\n",
        "  plot_roc_curve(classifiers,X_train_smote, y_train_smote,X_test,y_test)\n",
        "\n",
        "ml_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unzCxXjCpp-Q"
      },
      "source": [
        "#FINAL OBSERVATION:\n",
        "\n",
        "---\n",
        "### BASELINE MODELS:\n",
        "---\n",
        "\n",
        "🟪 **ACCURACY**:\n",
        "\n",
        "From above analysis we can show the overall HIGH ACCURACY of all the models.\n",
        "\n",
        "🟪 **PRECISION AND RECALL:**\n",
        "\n",
        "When it comes to Precision and Recall, only Decision Tree had made a good trade off between them as precisiona nd recall both are nicely balanced with good true positive and true negative captures. Where as the other models have a major gap between precision and recall as they are likely to be overfitted.\n",
        "\n",
        "✅ **BEST PERFORMING MODEL:**\n",
        "\n",
        "Decision Tree shows a very good performance of accuracy, precision and recall than other models with accuracy (0.88), precision (0.79) and recall (0.75) having roc curve of 0.84 which is approx a perfect curve.\n",
        "\n",
        "---\n",
        "### RESAMPLING WITH SMOTE:\n",
        "---\n",
        "\n",
        "🟪 **ACCURACY**:\n",
        "\n",
        "There is a slight decrease in accuracy after we done the oversampling and make the dataset balance with class. This decrease in accuracy predicted chances of recovering from the overfitting which we faced in Baseline model accuracies.\n",
        "\n",
        "🟪 **PRECISION AND RECALL:**\n",
        "\n",
        "The precison and recall for model execution after SMOTE shows the increase in recall that shows that the minority class is better recognized by oversampling.\n",
        "Decison Tree and Random Forest have performed best after SMOTE. The good balance between precision and recall captures the ability of model to generalize well on testing dataset\n",
        "\n",
        "✅ **BEST PERFORMING MODEL:**\n",
        "\n",
        "Random Forest Classifier shows a very good performance of accuracy, precision and recall than other models with accuracy (0.98), precision (0.78) and recall (0.78) having roc curve 90%.\n",
        "\n",
        "---\n",
        "### RESAMPLING WITH SMOTE + PCA:\n",
        "---\n",
        "\n",
        "Its good to check our model with feature selection as well as to remove the correlated features or select the feature only which gives us high variance in maximum. After PCA we found that 99% of varinace in dataset can be predicted from 2 features, so we used those two features to predict the model.\n",
        "\n",
        "🟪 **ACCURACY**:\n",
        "\n",
        "AFter PCA, the SMOTE values decreases the accuracy of the majority models except Guassian Naive Bayes, whose accuracy remain same as simple SMOTE. But overall PCA have decreased the model accuracy suggesting that there might be a loss of information.\n",
        "\n",
        "🟪 **PRECISION AND RECALL:**\n",
        "\n",
        "The precison and recall for model execution after SMOTE and PCA also shows the decrease as comapre to Baseline and simple SMOTE inidicates the PCA might make our model less computationally complex and stabilize the variance but it compromises on model sensitivity.\n",
        "\n",
        "✅ **BEST PERFORMING MODEL:**\n",
        "\n",
        "Guassian Naive Bayes shows a very good performance of accuracy, precision and recall than other models with accuracy (0.77), precision (0.59) and recall (0.59). But we will not consider these results as the accurate one due to our initial 2 approaches of finding best models. Also it may be the case where GN Bayes have made random guess as its precision and recall is almost 50-50%.\n",
        "\n",
        "❌ **POOR PERFORMING MODEL:**\n",
        "\n",
        "Decision Tree, KNN, Random Forest, SVM all perform quite low in case of PCA as compare to GN Bayes.\n",
        "\n",
        "\n",
        "---\n",
        "### BAGGING WITH RANDOM FOREST:\n",
        "---\n",
        "\n",
        "Bagging Classifier shows the best results in terms of precision (0.77), recall (0.85) ad area under curve (0.90) which is almost similar to basleine model with hypertuning and SMOTE, it tells the ability to separate from positve and negative classes that ultimately gives excellent performance as compare to all previous models.\n",
        "\n",
        "\n",
        "---\n",
        "### BOOSTING WITH DECISION TREES ADABOOST:\n",
        "---\n",
        "\n",
        "For boosting, adaboost classifer with weak decison tree learning perform well with f1-score (0.78) which shows the approximately good results for predicitng true positive and false posiitves. The area under curve (0.87) which tells the ability to separate from positve and negative classes that ultimately gives excellent performance as compare to all previous models and same like bagging algorithm.\n",
        "\n",
        "---\n",
        "### One Class SVM:\n",
        "---\n",
        "\n",
        "OneClass SVM predicts the performance metrics of outlier/anamoly class \"Revenue\" which is quite low as compare to other class. As a matter of revenue earned or not recall matters most which is low in our case of only 35% which is quite critical as it had not correctly classified the actual revenue earned or not from shopping. The f1-score and roc (0.37) is also very low indicates that the model just randomly guesses for the outliers class (i.e. Revenue=1)\n",
        "\n",
        "---\n",
        "### CONCLUSION:\n",
        "---\n",
        "\n",
        "✅**RANDOM FOREST CLASSIFER (SMOTE)** is the BEST choice of model so far if we are not using bagging and boosting models, as its accuracy (0.88), precision (0.78) and recall (0.81) is quite consistent and reliable for such Revenue Earning Predictions.\n",
        "\n",
        "✨ It minimizes the False Negative (i.e. accurately predicted revenue earned cases as revenue recieved) with HIGH RECALL than any other model.\n",
        "\n",
        "✨ It maximizes the True Positive and minimizes the False Positive with HIGH PRECISION than any other model.\n",
        "\n",
        "✅**BAGGING AND BOOSTING MODELS: (ENSEMBLE METHODS)**\n",
        "\n",
        "But when you are considering bagging or boosting model, you can go with any as they both perform equally well in predicting the bankruptcy cases even more than simple random forest classifer.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}